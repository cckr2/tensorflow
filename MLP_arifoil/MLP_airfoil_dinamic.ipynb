{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,data_file, log_file, save_file, hidden_node_num):\n",
    "        #Define instance variable\n",
    "        self.__data_file = data_file\n",
    "        self.__log_file = log_file\n",
    "        self.__save_file = save_file\n",
    "        self.__log = None\n",
    "\n",
    "        self.__hidden_node_num = hidden_node_num\n",
    "        self.__hidden_layer_num = len(self.__hidden_node_num)\n",
    "        \n",
    "        self.x_train_data = None\n",
    "        self.y_train_data = None\n",
    "        self.x_test_data = None\n",
    "        self.y_test_data = None\n",
    "        self.__x_data_len = None\n",
    "        \n",
    "        self.__y = None\n",
    "        self.__x = None\n",
    "        \n",
    "        self.__w = []\n",
    "        self.__b = []\n",
    "        self.__layer = []\n",
    "        self.__cost = None\n",
    "        self.__min_cost = 99999999\n",
    "        self.__min_acc = 99999999\n",
    "        \n",
    "        self.__y_out = None\n",
    "        self.__train = None\n",
    "        self.__sess = None\n",
    "        self.__saver = None\n",
    "        \n",
    "        self.__loadData()\n",
    "        self.__init_model()\n",
    "        \n",
    "    def __loadData(self):\n",
    "        #Read data file\n",
    "        data_file_name = self.__data_file\n",
    "        xy = np.genfromtxt(data_file_name, dtype='float32')\n",
    "        \n",
    "        #Shuffle data\n",
    "        np.random.shuffle(xy)\n",
    "        \n",
    "        #Data Split into train data and test data\n",
    "        all_data_num = xy[:,1:-1].shape[0]\n",
    "        train_data_num = int(all_data_num * 95 /100)\n",
    "        test_data_num = all_data_num - train_data_num\n",
    "        self.x_train_data =  xy[:train_data_num,1:-1]\n",
    "        self.y_train_data =  xy[:train_data_num,-1]\n",
    "        self.x_test_data =  xy[train_data_num:,1:-1]\n",
    "        self.y_test_data =  xy[train_data_num:,-1]\n",
    "\n",
    "        #Transpose Matrix having x_data\n",
    "        self.x_train_data = self.x_train_data.transpose()\n",
    "        self.x_test_data = self.x_test_data.transpose()\n",
    "\n",
    "        #Calculate num of variable\n",
    "        self.__x_data_len = len(self.x_train_data)\n",
    "        \n",
    "    def __init_model(self):\n",
    "        self.__x = tf.placeholder(dtype=tf.float32)\n",
    "        self.__y = tf.placeholder(dtype=tf.float32)\n",
    "        \n",
    "        # first layer\n",
    "        self.__w.append(tf.Variable(tf.random_normal([self.__hidden_node_num[0], self.__x_data_len]), name=\"w0\"))\n",
    "        self.__b.append(tf.Variable(tf.random_normal([self.__hidden_node_num[0],1]), name=\"b0\"))\n",
    "       \n",
    "        # add hidden layers (variable number)\n",
    "        for i in range(1,self.__hidden_layer_num):\n",
    "            wName = \"w\" + str(i)\n",
    "            bName = \"b\" + str(i)\n",
    "            self.__w.append(tf.Variable(tf.random_normal([self.__hidden_node_num[i], self.__hidden_node_num[i-1]]), name=wName))\n",
    "            self.__b.append(tf.Variable(tf.random_normal([self.__hidden_node_num[i],1]), name=bName))\n",
    "        \n",
    "        # add final layer\n",
    "        wName = \"w\" + str(self.__hidden_layer_num)\n",
    "        bName = \"b\" + str(self.__hidden_layer_num)\n",
    "        self.__w.append(tf.Variable(tf.random_normal([1, self.__hidden_node_num[-1]]), name=wName))\n",
    "        self.__b.append(tf.Variable(tf.random_normal([1],1), name=bName))\n",
    "        \n",
    "        # define model\n",
    "        self.__layer.append(tf.nn.sigmoid(tf.matmul( self.__w[0],self.__x) + self.__b[0]))\n",
    "        for i in range(1,self.__hidden_layer_num):\n",
    "            self.__layer.append(tf.nn.sigmoid(tf.matmul( self.__w[i],self.__layer[i-1]) + self.__b[i]))\n",
    "        self.__y_out = tf.matmul(self.__w[-1],self.__layer[-1]) + self.__b[-1]\n",
    "        \n",
    "        # setup cost function and optimizer\n",
    "        cost = 0\n",
    "        learning_rate = 0.001\n",
    "        if(cost == 0)\n",
    "            # Least Squares\n",
    "            self.__cost = tf.reduce_mean(tf.square(self.__y_out-  self.__y))\n",
    "        elif(cost == 1)\n",
    "            # L2 Loss\n",
    "            self.__cost = tf.nn.l2_loss(self.__y_out-self.__y)\n",
    "            \n",
    "        if(Optimizer == 0)\n",
    "            # Gradient Descent\n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif(Optimizer == 1)\n",
    "            # & Adam\n",
    "            opt= tf.train.AdamOptimizer(learning_rate)\n",
    "        elif(Optimizer == 2)\n",
    "            # Adagrad\n",
    "            opt= tf.train.tf.train.AdagradOptimizer(learning_rate)\n",
    "        elif(Optimizer == 3)\n",
    "            # Momentum \n",
    "            # MomentumOptimizer with 0.9 momentum is very standard and works well. \n",
    "            # The drawback is that you have to find yourself the best learning rate.\n",
    "            opt= tf.train.tf.train.MomentumOptimizer(learning_rate,momentum=0.9)    \n",
    "        elif(Optimizer == 4)\n",
    "            # Adadelta\n",
    "            # If you really want to use Adadelta, use the parameters (learning_rate=1., rho=0.95, epsilon=1e-6). \n",
    "            # A bigger epsilon will help at the start, but be prepared to wait a bit longer than with other optimizers to see convergence.\n",
    "            opt= tf.train.AdadeltaOptimizer(1., 0.95, 1e-6)\n",
    "        elif(Optimizer == 5)\n",
    "            # RMSProp\n",
    "            # The results are less dependent on a good learning rate. This algorithm is very similar to Adadelta\n",
    "            opt= tf.train.RMSPropOptimizer(learning_rate)\n",
    "            \n",
    "            \n",
    "        self.__train = opt.minimize(self.__cost)\n",
    "    \n",
    "    def init_sess(self):\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.__sess = tf.Session()\n",
    "        self.__sess.run(init)\n",
    "        \n",
    "        if self.__save_file != None:\n",
    "            self.__saver = tf.train.Saver()\n",
    "            \n",
    "        if self.__log_file != None:\n",
    "            self.__log = open(self.__log_file,'w')\n",
    "            self.__log.write(\"Step\\tTraning Cost\\tTest Accuracy\\n\")\n",
    "            self.__log.close()\n",
    "        \n",
    "    def model_train(self):\n",
    "        self.__sess.run(self.__train,feed_dict={self.__x: self.x_test_data, self.__y: self.y_test_data})\n",
    "        \n",
    "    def __model_save(self,cost,acc):\n",
    "        if self.__save_file != None:\n",
    "            if self.__min_cost > cost:\n",
    "                self.__min_cost = cost\n",
    "                self.__saver.save(self.__sess, self.__save_file, write_meta_graph=False)\n",
    "            elif self.__min_cost == cost:\n",
    "                if self.__min_acc > acc:\n",
    "                    self.__min_acc = acc\n",
    "                    self.__saver.save(self.__sess, self.__save_file, write_meta_graph=False)\n",
    "\n",
    "    def log_write(self, num):\n",
    "        if self.__log_file != None:\n",
    "            predic = self.__sess.run(self.__y_out,feed_dict={self.__x: self.x_test_data})\n",
    "            acc = np.mean((self.y_test_data- predic) * 100 / predic)\n",
    "            ccost = self.__sess.run(self.__cost,feed_dict={self.__x: self.x_test_data,self.__y: self.y_test_data})\n",
    "            self.__log = open(self.__log_file,'a')\n",
    "            self.__log.write(str(num) + \"\\t\")\n",
    "            self.__log.write(str(ccost)+ \"\\t\")\n",
    "            self.__log.write(str(acc)+ \"\\n\")\n",
    "            self.__log.close()\n",
    "            \n",
    "            self.__model_save(ccost,acc)\n",
    "        \n",
    "    def close(self):\n",
    "        self.__sess.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_node_num = [10,10,10]\n",
    "# hidden_node_num = [10]\n",
    "train_num = 5\n",
    "print_inter = 1\n",
    "alpha_num = 5\n",
    "start_alpha = 0\n",
    "end_alpha = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowDatetime = datetime.datetime.now().strftime('%m%d_%H%M')\n",
    "logDir = \"log/\" + nowDatetime\n",
    "saveDir= \"model/\" + nowDatetime\n",
    "if not os.path.exists(logDir):\n",
    "    os.makedirs(logDir)\n",
    "    \n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)    \n",
    "\n",
    "model_array = []\n",
    "\n",
    "for i in range(start_alpha,end_alpha+1):\n",
    "    dataPath = \"data/data_Alpha_\" + str(i) + \".txt\"\n",
    "    logPath = logDir + \"/log_\" + str(i) +\".txt\"\n",
    "    savePath = saveDir  + \"/model_\" + str(i)\n",
    "    model_array.append(MLP(dataPath, logPath, savePath,hidden_node_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Start\n",
      "Step :  0\n",
      "Step :  1\n",
      "Step :  2\n",
      "Step :  3\n",
      "Step :  4\n",
      "Traning Finish\n",
      "Traning Finish\n"
     ]
    }
   ],
   "source": [
    "print(\"Traning Start\")\n",
    "\n",
    "for i in range(alpha_num):\n",
    "    model_array[i].init_sess()\n",
    "    \n",
    "for step in range(train_num):\n",
    "    for i in range(alpha_num):\n",
    "        model_array[i].model_train()\n",
    "    if step%print_inter == 0:\n",
    "        print(\"Step : \",step)\n",
    "        for i in range(alpha_num):\n",
    "            model_array[i].log_write(step)\n",
    "\n",
    "for i in range(alpha_num):            \n",
    "    model_array[i].log_write(step)      \n",
    "    model_array[i].close()\n",
    "    \n",
    "print(\"Traning Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
